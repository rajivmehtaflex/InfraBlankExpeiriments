version: 1
default_environment: dev
plugins:
  extractors:
  - name: tap-csv
    variant: meltanolabs
    pip_url: git+https://github.com/MeltanoLabs/tap-csv.git
    config:
      files:
      - entity: customers
        path: data/customers.csv
        keys: [id]
      - entity: orders
        path: data/orders.csv
        keys: [id]
      - entity: payments
        path: data/payments.csv
        keys: [id]
  loaders:
  - name: target-postgres
    variant: transferwise
    pip_url: pipelinewise-target-postgres
  - name: target-jsonl
    variant: andyh1203
    pip_url: target-jsonl
  orchestrators:
  - name: airflow
    pip_url: apache-airflow==2.1.2 --constraint https://raw.githubusercontent.com/apache/airflow/constraints-2.1.2/constraints-${MELTANO__PYTHON_VERSION}.txt
  transformers:
  - name: dbt
    pip_url: "dbt-core~=1.0.0 dbt-postgres~=1.0.0 dbt-redshift~=1.0.0 dbt-snowflake~=1.0.0\
      \ dbt-bigquery~=1.0.0\n"
  files:
  - name: airflow
    pip_url: git+https://gitlab.com/pnadolny13/files-airflow-dbt.git
  - name: dbt
    pip_url: git+https://gitlab.com/meltano/files-dbt.git@config-version-2
environments:
- name: dev
  config:
    plugins:
      loaders:
      - name: target-postgres
        config:
          user: meltano
          dbname: warehouse
          default_target_schema: public
      transformers:
      - name: dbt
        config:
          target: postgres
          source_schema: public
  env:
    PG_ADDRESS: localhost
    PG_PORT: '5432'
    PG_USERNAME: meltano
    PG_DATABASE: warehouse
    AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL: '5'
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
- name: staging
- name: prod
project_id: 3cebe36d-55f0-4ad9-9605-f6fcc6c22666
